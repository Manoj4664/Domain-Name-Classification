{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The script begins by installing and importing all necessary libraries for data handling, feature extraction, and machine learning.\n",
        "\n",
        "It downloads an English word list from NLTK, which is later used to detect meaningful words inside domain names.\n",
        "\n",
        "It attempts to load a dataset of the top one million popular domains to use as a reference for identifying whether a domain is well-known or suspicious.\n",
        "\n",
        "A list of suspicious or risky top-level domains (TLDs) is defined, such as .xyz, .tk, and .ml.\n",
        "\n",
        "Several feature extraction functions are defined. These compute statistical and linguistic measures such as Shannon entropy, n-gram diversity, repeated character patterns, word matching ratios, sliding dictionary word matches, longest dictionary word length, character distribution irregularity, vowel-consonant alternation, and lexical complexity.\n",
        "\n",
        "A central feature extraction function combines all these measures to generate a complete feature set for each domain. It produces values such as domain length, digit count, maximum consecutive digits, vowel and consonant counts, entropy score, n-gram scores, pronounceability, lexical complexity, repetition patterns, unique character count, and TLD-based indicators.\n",
        "\n",
        "The training dataset containing domain names and their labels (benign or malicious) is loaded.\n",
        "\n",
        "Each domain in the dataset is transformed into a feature vector using the extraction functions, creating an enriched training dataset.\n",
        "\n",
        "The dataset is then split into input features and target labels.\n",
        "\n",
        "An XGBoost classifier is configured and trained using the extracted features and known labels.\n",
        "\n",
        "Once training is complete, the model is saved to disk as a .pkl file so that it can be reused later for classifying new domains."
      ],
      "metadata": {
        "id": "nDiAIyaQggyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =========================\n",
        "# PART 1: TRAIN & SAVE MODEL\n",
        "# =========================\n",
        "\n",
        "!pip install xgboost nltk joblib pandas numpy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import math\n",
        "from collections import Counter\n",
        "from xgboost import XGBClassifier\n",
        "import joblib\n",
        "from nltk.corpus import words\n",
        "from nltk import download\n",
        "\n",
        "# ------------------------\n",
        "# SETUP (For Google Colab)\n",
        "# ------------------------\n",
        "download('words')\n",
        "english_words = set(words.words())\n",
        "\n",
        "# If you have 'top-1m.csv', upload via Colab file upload first\n",
        "try:\n",
        "    top1m = pd.read_csv('top-1m.csv', header=None)\n",
        "    popular_domains = set(top1m.iloc[:, -1].str.lower())\n",
        "except Exception:\n",
        "    popular_domains = set()\n",
        "\n",
        "bad_tlds = ['xyz', 'click', 'top', 'gq', 'tk', 'ml', 'cf']\n",
        "\n",
        "# ------------------------\n",
        "# Feature Computation\n",
        "# ------------------------\n",
        "def shannon_entropy(s):\n",
        "    freq = Counter(s)\n",
        "    probs = [f / len(s) for f in freq.values()]\n",
        "    return -sum(p * math.log2(p) for p in probs)\n",
        "\n",
        "def ngram_score(s, n):\n",
        "    ng = [s[i:i+n] for i in range(len(s)-n+1)]\n",
        "    return len(set(ng)) / (len(ng) or 1)\n",
        "\n",
        "def count_repeats(s):\n",
        "    return len(re.findall(r'(.)\\1+', s))\n",
        "\n",
        "def compute_word_match_ratio(s):\n",
        "    tokens = re.split(r'\\W+', s)\n",
        "    matched = sum(1 for t in tokens if t in english_words and len(t) > 2)\n",
        "    return matched / len(tokens) if tokens else 0\n",
        "\n",
        "def sliding_word_ratio(s):\n",
        "    matches = total = 0\n",
        "    for size in range(3, 10):\n",
        "        for i in range(len(s) - size + 1):\n",
        "            sub = s[i:i+size]\n",
        "            total += 1\n",
        "            if sub in english_words:\n",
        "                matches += 1\n",
        "    return matches / total if total else 0\n",
        "\n",
        "def longest_dict_word(s):\n",
        "    tokens = re.split(r'\\W+', s)\n",
        "    lengths = [len(t) for t in tokens if t in english_words]\n",
        "    return max(lengths) if lengths else 0\n",
        "\n",
        "def char_distribution_std(s):\n",
        "    vals = np.array(list(Counter(s).values()))\n",
        "    return float(np.std(vals))\n",
        "\n",
        "def vowel_consonant_alternation(s):\n",
        "    vc = ''.join('v' if c in 'aeiou' else 'c' if c.isalpha() else '' for c in s)\n",
        "    return sum(1 for i in range(1, len(vc)) if vc[i] != vc[i-1])\n",
        "\n",
        "def compute_lexical_complexity(s):\n",
        "    cons_clusters = re.findall(r'[^aeiou]{3,}', s)\n",
        "    alternations = sum(\n",
        "        1 for i in range(1, len(s))\n",
        "        if s[i].isalpha() and s[i-1].isalpha() and (s[i].isalpha() != s[i-1].isalpha())\n",
        "    )\n",
        "    return len(cons_clusters) + alternations\n",
        "\n",
        "def compute_domain_features(domain):\n",
        "    dom = domain.lower()\n",
        "    vowels = 'aeiou'\n",
        "    consonants = 'bcdfghjklmnpqrstvwxyz'\n",
        "    length = len(dom)\n",
        "    digit_count = sum(c.isdigit() for c in dom)\n",
        "    max_consec_digits = max((len(g) for g in re.findall(r'\\d+', dom)), default=0)\n",
        "    vowel_count = sum(c in vowels for c in dom)\n",
        "    consonant_count = sum(c in consonants for c in dom)\n",
        "    entropy = shannon_entropy(dom)\n",
        "    word_match_ratio = compute_word_match_ratio(dom)\n",
        "    sliding_ratio = sliding_word_ratio(dom)\n",
        "    longest_word = longest_dict_word(dom)\n",
        "    bigram_score = ngram_score(dom, 2)\n",
        "    trigram_score = ngram_score(dom, 3)\n",
        "    pronounceability = vowel_count / length if length else 0\n",
        "    unique_chars = len(set(dom))\n",
        "    dist_std = char_distribution_std(dom)\n",
        "    vowel_consonant_alt = vowel_consonant_alternation(dom)\n",
        "    repeat_chars = count_repeats(dom)\n",
        "    cons_vowel_ratio = consonant_count / vowel_count if vowel_count else consonant_count\n",
        "    has_hyphen = 1 if '-' in dom else 0\n",
        "    lexical_complexity = compute_lexical_complexity(dom)\n",
        "    tld = dom.split('.')[-1]\n",
        "    tld_common = 1 if tld in ['com', 'net', 'org', 'info', 'biz'] else 0\n",
        "    tld_bad = 1 if tld in bad_tlds else 0\n",
        "    privacy = 1 if dom in popular_domains else 0\n",
        "\n",
        "    return {\n",
        "        'domains': domain,\n",
        "        'Length': length,\n",
        "        'Digit_Count': digit_count,\n",
        "        'Max_Consec_Digits': max_consec_digits,\n",
        "        'Vowel_Count': vowel_count,\n",
        "        'Consonant_Count': consonant_count,\n",
        "        'Unique_Chars': unique_chars,\n",
        "        'Entropy': entropy,\n",
        "        'Dist_STD': dist_std,\n",
        "        'Word_Match_Ratio': word_match_ratio,\n",
        "        'Sliding_Word_Ratio': sliding_ratio,\n",
        "        'Longest_Word_Len': longest_word,\n",
        "        'Bigram_Score': bigram_score,\n",
        "        'Trigram_Score': trigram_score,\n",
        "        'Vowel_Consonant_Alt': vowel_consonant_alt,\n",
        "        'Pronounceability': pronounceability,\n",
        "        'Repeat_Chars': repeat_chars,\n",
        "        'Cons_Vowel_Ratio': cons_vowel_ratio,\n",
        "        'Has_Hyphen': has_hyphen,\n",
        "        'Lexical_Complexity': lexical_complexity,\n",
        "        'TLD_Common': tld_common,\n",
        "        'TLD_Bad_Score': tld_bad,\n",
        "        'Popular_Domain': privacy\n",
        "    }\n",
        "\n",
        "# ------------------------\n",
        "# TRAINING\n",
        "# ------------------------\n",
        "# Upload 'equal_extra_unique_combined_dataset.csv' in Colab\n",
        "train_df = pd.read_csv('m_1_unique_combined_dataset.csv')\n",
        "\n",
        "# Compute features\n",
        "augmented_train_data = []\n",
        "for idx, row in train_df.iterrows():\n",
        "    feats = compute_domain_features(row['domains'])\n",
        "    feats['label'] = row['label']\n",
        "    augmented_train_data.append(feats)\n",
        "\n",
        "augmented_train_df = pd.DataFrame(augmented_train_data)\n",
        "\n",
        "# Train model\n",
        "X_train = augmented_train_df.drop(columns=['domains', 'label'])\n",
        "y_train = augmented_train_df['label']\n",
        "\n",
        "model = XGBClassifier(n_estimators=200, max_depth=6, learning_rate=0.05, eval_metric='logloss')\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Training Completed.\")\n",
        "\n",
        "# Save model\n",
        "joblib.dump(model, 'm_1_xgb_dga_classifier.pkl')\n",
        "print(\"Model saved as 'm_1_xgb_dga_classifier.pkl'\")\n"
      ],
      "metadata": {
        "id": "LjzNag7Nf1Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The script starts by importing all necessary libraries for data handling, feature extraction, and model prediction.\n",
        "\n",
        "It downloads an English dictionary word list from NLTK to assist in word-based feature extraction.\n",
        "\n",
        "It attempts to load a list of the top one million domains to mark whether a domain is popular or not. If this file is unavailable, it proceeds with an empty list.\n",
        "\n",
        "A list of known suspicious TLDs (such as .xyz, .tk, .ml, etc.) is defined.\n",
        "\n",
        "A set of feature extraction functions are provided. These compute metrics such as:\n",
        "\n",
        "Shannon entropy (randomness of characters)\n",
        "\n",
        "N-gram scores (bigram and trigram diversity)\n",
        "\n",
        "Repeated character sequences\n",
        "\n",
        "Dictionary word match ratios (direct and sliding)\n",
        "\n",
        "Longest English word inside the domain\n",
        "\n",
        "Character distribution irregularity\n",
        "\n",
        "Vowelâ€“consonant alternations (pronounceability measure)\n",
        "\n",
        "Lexical complexity (unusual consonant clusters and alternations)\n",
        "\n",
        "A main function combines these metrics into a complete feature vector for any given domain. It includes structural features (length, digit count, max consecutive digits), lexical statistics (entropy, bigram/trigram scores, repetition, word ratios), and TLD-based indicators (common TLD, bad TLD, popular domain flag).\n",
        "\n",
        "The pre-trained XGBoost model (m_1_xgb_dga_classifier.pkl) is loaded from disk using Joblib.\n",
        "\n",
        "A confirmation message is printed to indicate that the model has been successfully loaded.\n",
        "\n",
        "The program enters an interactive loop, prompting the user to enter one or more domain names (comma-separated).\n",
        "\n",
        "If the user types \"q\", the program exits gracefully.\n",
        "\n",
        "For each input domain, the script computes its feature vector using the extraction functions.\n",
        "\n",
        "These features are converted into a DataFrame and passed to the model for classification.\n",
        "\n",
        "The model predicts the probability of the domain being malicious (DGA) versus benign.\n",
        "\n",
        "A probability threshold of 0.4 is applied:\n",
        "\n",
        "If probability â‰¥ 0.4 â†’ classified as DGA (malicious).\n",
        "\n",
        "Otherwise â†’ classified as Benign (normal).\n",
        "\n",
        "The prediction results are displayed in a neatly formatted table, showing:\n",
        "\n",
        "The input domain\n",
        "\n",
        "Predicted label (0 = Benign, 1 = DGA)\n",
        "\n",
        "Model probability score\n",
        "\n",
        "Final classification (Benign/DGA)\n",
        "\n",
        "After showing results, the loop continues, allowing the user to test more domains until they quit."
      ],
      "metadata": {
        "id": "2xB6RkyTgxcT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-e5-_WRRQwC"
      },
      "outputs": [],
      "source": [
        "# ========================= Model-1======================\n",
        "# PART 2: LOAD MODEL & PREDICT INTERACTIVELY\n",
        "# =========================\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import math\n",
        "from collections import Counter\n",
        "from xgboost import XGBClassifier\n",
        "import joblib\n",
        "from nltk.corpus import words\n",
        "from nltk import download\n",
        "\n",
        "# Setup\n",
        "download('words')\n",
        "english_words = set(words.words())\n",
        "\n",
        "# Load external resources\n",
        "try:\n",
        "    top1m = pd.read_csv('top-1m.csv', header=None)\n",
        "    popular_domains = set(top1m.iloc[:, -1].str.lower())\n",
        "except Exception:\n",
        "    popular_domains = set()\n",
        "\n",
        "bad_tlds = ['xyz', 'click', 'top', 'gq', 'tk', 'ml', 'cf']\n",
        "\n",
        "# =========================\n",
        "# Feature Functions\n",
        "# =========================\n",
        "def shannon_entropy(s):\n",
        "    freq = Counter(s)\n",
        "    probs = [f / len(s) for f in freq.values()]\n",
        "    return -sum(p * math.log2(p) for p in probs)\n",
        "\n",
        "def ngram_score(s, n):\n",
        "    ng = [s[i:i+n] for i in range(len(s)-n+1)]\n",
        "    return len(set(ng)) / (len(ng) or 1)\n",
        "\n",
        "def count_repeats(s):\n",
        "    return len(re.findall(r'(.)\\1+', s))\n",
        "\n",
        "def compute_word_match_ratio(s):\n",
        "    tokens = re.split(r'\\W+', s)\n",
        "    matched = sum(1 for t in tokens if t in english_words and len(t) > 2)\n",
        "    return matched / len(tokens) if tokens else 0\n",
        "\n",
        "def sliding_word_ratio(s):\n",
        "    matches = total = 0\n",
        "    for size in range(3, 10):\n",
        "        for i in range(len(s) - size + 1):\n",
        "            sub = s[i:i+size]\n",
        "            total += 1\n",
        "            if sub in english_words:\n",
        "                matches += 1\n",
        "    return matches / total if total else 0\n",
        "\n",
        "def longest_dict_word(s):\n",
        "    tokens = re.split(r'\\W+', s)\n",
        "    lengths = [len(t) for t in tokens if t in english_words]\n",
        "    return max(lengths) if lengths else 0\n",
        "\n",
        "def char_distribution_std(s):\n",
        "    vals = np.array(list(Counter(s).values()))\n",
        "    return float(np.std(vals))\n",
        "\n",
        "def vowel_consonant_alternation(s):\n",
        "    vc = ''.join('v' if c in 'aeiou' else 'c' if c.isalpha() else '' for c in s)\n",
        "    return sum(1 for i in range(1, len(vc)) if vc[i] != vc[i-1])\n",
        "\n",
        "def compute_lexical_complexity(s):\n",
        "    cons_clusters = re.findall(r'[^aeiou]{3,}', s)\n",
        "    alternations = sum(\n",
        "        1 for i in range(1, len(s))\n",
        "        if s[i].isalpha() and s[i-1].isalpha() and (s[i].isalpha() != s[i-1].isalpha())\n",
        "    )\n",
        "    return len(cons_clusters) + alternations\n",
        "\n",
        "def compute_domain_features(domain):\n",
        "    dom = domain.lower()\n",
        "    vowels = 'aeiou'\n",
        "    consonants = 'bcdfghjklmnpqrstvwxyz'\n",
        "    length = len(dom)\n",
        "    digit_count = sum(c.isdigit() for c in dom)\n",
        "    max_consec_digits = max((len(g) for g in re.findall(r'\\d+', dom)), default=0)\n",
        "    vowel_count = sum(c in vowels for c in dom)\n",
        "    consonant_count = sum(c in consonants for c in dom)\n",
        "    entropy = shannon_entropy(dom)\n",
        "    word_match_ratio = compute_word_match_ratio(dom)\n",
        "    sliding_ratio = sliding_word_ratio(dom)\n",
        "    longest_word = longest_dict_word(dom)\n",
        "    bigram_score = ngram_score(dom, 2)\n",
        "    trigram_score = ngram_score(dom, 3)\n",
        "    pronounceability = vowel_count / length if length else 0\n",
        "    unique_chars = len(set(dom))\n",
        "    dist_std = char_distribution_std(dom)\n",
        "    vowel_consonant_alt = vowel_consonant_alternation(dom)\n",
        "    repeat_chars = count_repeats(dom)\n",
        "    cons_vowel_ratio = consonant_count / vowel_count if vowel_count else consonant_count\n",
        "    has_hyphen = 1 if '-' in dom else 0\n",
        "    lexical_complexity = compute_lexical_complexity(dom)\n",
        "    tld = dom.split('.')[-1]\n",
        "    tld_common = 1 if tld in ['com', 'net', 'org', 'info', 'biz'] else 0\n",
        "    tld_bad = 1 if tld in bad_tlds else 0\n",
        "    privacy = 1 if dom in popular_domains else 0\n",
        "\n",
        "    return {\n",
        "        'domains': domain,\n",
        "        'Length': length,\n",
        "        'Digit_Count': digit_count,\n",
        "        'Max_Consec_Digits': max_consec_digits,\n",
        "        'Vowel_Count': vowel_count,\n",
        "        'Consonant_Count': consonant_count,\n",
        "        'Unique_Chars': unique_chars,\n",
        "        'Entropy': entropy,\n",
        "        'Dist_STD': dist_std,\n",
        "        'Word_Match_Ratio': word_match_ratio,\n",
        "        'Sliding_Word_Ratio': sliding_ratio,\n",
        "        'Longest_Word_Len': longest_word,\n",
        "        'Bigram_Score': bigram_score,\n",
        "        'Trigram_Score': trigram_score,\n",
        "        'Vowel_Consonant_Alt': vowel_consonant_alt,\n",
        "        'Pronounceability': pronounceability,\n",
        "        'Repeat_Chars': repeat_chars,\n",
        "        'Cons_Vowel_Ratio': cons_vowel_ratio,\n",
        "        'Has_Hyphen': has_hyphen,\n",
        "        'Lexical_Complexity': lexical_complexity,\n",
        "        'TLD_Common': tld_common,\n",
        "        'TLD_Bad_Score': tld_bad,\n",
        "        'Popular_Domain': privacy\n",
        "    }\n",
        "\n",
        "# =========================\n",
        "# Load Model\n",
        "# =========================\n",
        "model = joblib.load('m_1_xgb_dga_classifier.pkl')\n",
        "print(\"âœ… Model loaded successfully.\")\n",
        "\n",
        "# =========================\n",
        "# Interactive Predictions\n",
        "# =========================\n",
        "print(\"\\nðŸ”® Enter domains for prediction (comma-separated). Type 'q' to quit.\\n\")\n",
        "\n",
        "threshold = 0.4\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Enter domain(s): \").strip()\n",
        "\n",
        "    if user_input.lower() == \"q\":\n",
        "        print(\"ðŸ‘‹ Exiting program.\")\n",
        "        break\n",
        "\n",
        "    domains = [d.strip() for d in user_input.split(\",\") if d.strip()]\n",
        "\n",
        "    if not domains:\n",
        "        print(\"âš ï¸ No valid domains entered. Try again.\")\n",
        "        continue\n",
        "\n",
        "    test_features = [compute_domain_features(d) for d in domains]\n",
        "    test_df = pd.DataFrame(test_features)\n",
        "    X_test = test_df.drop(columns=['domains'])\n",
        "\n",
        "    pred_probs = model.predict_proba(X_test)[:, 1]\n",
        "    pred_labels = (pred_probs >= threshold).astype(int)\n",
        "\n",
        "    results = pd.DataFrame({\n",
        "        'Domain': domains,\n",
        "        'Predicted_Label': pred_labels,\n",
        "        'Probability': pred_probs,\n",
        "        'Classification': [\"Benign\" if l == 0 else \"DGA\" for l in pred_labels]\n",
        "    })\n",
        "\n",
        "    print(\"\\n===== Prediction Results =====\")\n",
        "    print(results.to_string(index=False))\n",
        "    print(\"\\n\")\n"
      ]
    }
  ]
}