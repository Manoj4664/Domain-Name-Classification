{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-e5-_WRRQwC"
      },
      "outputs": [],
      "source": [
        "# ========================= Model-1======================\n",
        "# PART 2: LOAD MODEL & PREDICT INTERACTIVELY\n",
        "# =========================\n",
        "\n",
        "!pip install xgboost nltk joblib pandas numpy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import math\n",
        "from collections import Counter\n",
        "from xgboost import XGBClassifier\n",
        "import joblib\n",
        "from nltk.corpus import words\n",
        "from nltk import download\n",
        "\n",
        "# Setup\n",
        "download('words')\n",
        "english_words = set(words.words())\n",
        "\n",
        "# Load external resources\n",
        "try:\n",
        "    top1m = pd.read_csv('top-1m.csv', header=None)\n",
        "    popular_domains = set(top1m.iloc[:, -1].str.lower())\n",
        "except Exception:\n",
        "    popular_domains = set()\n",
        "\n",
        "bad_tlds = ['xyz', 'click', 'top', 'gq', 'tk', 'ml', 'cf']\n",
        "\n",
        "# =========================\n",
        "# Feature Functions\n",
        "# =========================\n",
        "def shannon_entropy(s):\n",
        "    freq = Counter(s)\n",
        "    probs = [f / len(s) for f in freq.values()]\n",
        "    return -sum(p * math.log2(p) for p in probs)\n",
        "\n",
        "def ngram_score(s, n):\n",
        "    ng = [s[i:i+n] for i in range(len(s)-n+1)]\n",
        "    return len(set(ng)) / (len(ng) or 1)\n",
        "\n",
        "def count_repeats(s):\n",
        "    return len(re.findall(r'(.)\\1+', s))\n",
        "\n",
        "def compute_word_match_ratio(s):\n",
        "    tokens = re.split(r'\\W+', s)\n",
        "    matched = sum(1 for t in tokens if t in english_words and len(t) > 2)\n",
        "    return matched / len(tokens) if tokens else 0\n",
        "\n",
        "def sliding_word_ratio(s):\n",
        "    matches = total = 0\n",
        "    for size in range(3, 10):\n",
        "        for i in range(len(s) - size + 1):\n",
        "            sub = s[i:i+size]\n",
        "            total += 1\n",
        "            if sub in english_words:\n",
        "                matches += 1\n",
        "    return matches / total if total else 0\n",
        "\n",
        "def longest_dict_word(s):\n",
        "    tokens = re.split(r'\\W+', s)\n",
        "    lengths = [len(t) for t in tokens if t in english_words]\n",
        "    return max(lengths) if lengths else 0\n",
        "\n",
        "def char_distribution_std(s):\n",
        "    vals = np.array(list(Counter(s).values()))\n",
        "    return float(np.std(vals))\n",
        "\n",
        "def vowel_consonant_alternation(s):\n",
        "    vc = ''.join('v' if c in 'aeiou' else 'c' if c.isalpha() else '' for c in s)\n",
        "    return sum(1 for i in range(1, len(vc)) if vc[i] != vc[i-1])\n",
        "\n",
        "def compute_lexical_complexity(s):\n",
        "    cons_clusters = re.findall(r'[^aeiou]{3,}', s)\n",
        "    alternations = sum(\n",
        "        1 for i in range(1, len(s))\n",
        "        if s[i].isalpha() and s[i-1].isalpha() and (s[i].isalpha() != s[i-1].isalpha())\n",
        "    )\n",
        "    return len(cons_clusters) + alternations\n",
        "\n",
        "def compute_domain_features(domain):\n",
        "    dom = domain.lower()\n",
        "    vowels = 'aeiou'\n",
        "    consonants = 'bcdfghjklmnpqrstvwxyz'\n",
        "    length = len(dom)\n",
        "    digit_count = sum(c.isdigit() for c in dom)\n",
        "    max_consec_digits = max((len(g) for g in re.findall(r'\\d+', dom)), default=0)\n",
        "    vowel_count = sum(c in vowels for c in dom)\n",
        "    consonant_count = sum(c in consonants for c in dom)\n",
        "    entropy = shannon_entropy(dom)\n",
        "    word_match_ratio = compute_word_match_ratio(dom)\n",
        "    sliding_ratio = sliding_word_ratio(dom)\n",
        "    longest_word = longest_dict_word(dom)\n",
        "    bigram_score = ngram_score(dom, 2)\n",
        "    trigram_score = ngram_score(dom, 3)\n",
        "    pronounceability = vowel_count / length if length else 0\n",
        "    unique_chars = len(set(dom))\n",
        "    dist_std = char_distribution_std(dom)\n",
        "    vowel_consonant_alt = vowel_consonant_alternation(dom)\n",
        "    repeat_chars = count_repeats(dom)\n",
        "    cons_vowel_ratio = consonant_count / vowel_count if vowel_count else consonant_count\n",
        "    has_hyphen = 1 if '-' in dom else 0\n",
        "    lexical_complexity = compute_lexical_complexity(dom)\n",
        "    tld = dom.split('.')[-1]\n",
        "    tld_common = 1 if tld in ['com', 'net', 'org', 'info', 'biz'] else 0\n",
        "    tld_bad = 1 if tld in bad_tlds else 0\n",
        "    privacy = 1 if dom in popular_domains else 0\n",
        "\n",
        "    return {\n",
        "        'domains': domain,\n",
        "        'Length': length,\n",
        "        'Digit_Count': digit_count,\n",
        "        'Max_Consec_Digits': max_consec_digits,\n",
        "        'Vowel_Count': vowel_count,\n",
        "        'Consonant_Count': consonant_count,\n",
        "        'Unique_Chars': unique_chars,\n",
        "        'Entropy': entropy,\n",
        "        'Dist_STD': dist_std,\n",
        "        'Word_Match_Ratio': word_match_ratio,\n",
        "        'Sliding_Word_Ratio': sliding_ratio,\n",
        "        'Longest_Word_Len': longest_word,\n",
        "        'Bigram_Score': bigram_score,\n",
        "        'Trigram_Score': trigram_score,\n",
        "        'Vowel_Consonant_Alt': vowel_consonant_alt,\n",
        "        'Pronounceability': pronounceability,\n",
        "        'Repeat_Chars': repeat_chars,\n",
        "        'Cons_Vowel_Ratio': cons_vowel_ratio,\n",
        "        'Has_Hyphen': has_hyphen,\n",
        "        'Lexical_Complexity': lexical_complexity,\n",
        "        'TLD_Common': tld_common,\n",
        "        'TLD_Bad_Score': tld_bad,\n",
        "        'Popular_Domain': privacy\n",
        "    }\n",
        "\n",
        "# =========================\n",
        "# Load Model\n",
        "# =========================\n",
        "model = joblib.load('m_1_xgb_dga_classifier.pkl')\n",
        "print(\"âœ… Model loaded successfully.\")\n",
        "\n",
        "# =========================\n",
        "# Interactive Predictions\n",
        "# =========================\n",
        "print(\"\\nðŸ”® Enter domains for prediction (comma-separated). Type 'q' to quit.\\n\")\n",
        "\n",
        "threshold = 0.4\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Enter domain(s): \").strip()\n",
        "\n",
        "    if user_input.lower() == \"q\":\n",
        "        print(\"ðŸ‘‹ Exiting program.\")\n",
        "        break\n",
        "\n",
        "    domains = [d.strip() for d in user_input.split(\",\") if d.strip()]\n",
        "\n",
        "    if not domains:\n",
        "        print(\"âš ï¸ No valid domains entered. Try again.\")\n",
        "        continue\n",
        "\n",
        "    test_features = [compute_domain_features(d) for d in domains]\n",
        "    test_df = pd.DataFrame(test_features)\n",
        "    X_test = test_df.drop(columns=['domains'])\n",
        "\n",
        "    pred_probs = model.predict_proba(X_test)[:, 1]\n",
        "    pred_labels = (pred_probs >= threshold).astype(int)\n",
        "\n",
        "    results = pd.DataFrame({\n",
        "        'Domain': domains,\n",
        "        'Predicted_Label': pred_labels,\n",
        "        'Probability': pred_probs,\n",
        "        'Classification': [\"Benign\" if l == 0 else \"DGA\" for l in pred_labels]\n",
        "    })\n",
        "\n",
        "    print(\"\\n===== Prediction Results =====\")\n",
        "    print(results.to_string(index=False))\n",
        "    print(\"\\n\")\n"
      ]
    }
  ]
}